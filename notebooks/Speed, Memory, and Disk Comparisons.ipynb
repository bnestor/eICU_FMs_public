{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652c54c0",
   "metadata": {},
   "source": [
    "# Speed, Memory, and Disk Comparisons\n",
    "\n",
    "In this notebook, we'll offer some rough comparisons of the computational performance implications of ESGPT vs. other competing pipelines. We'll focus these comparisons on several metrics:\n",
    "  1. The time, runtime memory, and final disk space required to construct, pre-process, and store an ESGPT dataset relative to other pipelines, where applicable.\n",
    "  2. The initialization time, iteration speed, and GPU memory costs for producing batches of data within the ESGPT framework vs. other systems.\n",
    "  \n",
    "In particular, we'll compare (or justify why they are inappropriate comparators) against the following pipelines:\n",
    "  1. TemporAI\n",
    "  2. OMOP-Learn\n",
    "  3. FIDDLE\n",
    "  4. MIMIC-Extract\n",
    "  \n",
    "We'll make these comparisons leveraging the synthetic data distributed with ESGPT's sample tutorial, but this code can also be ported to any other dataset to run these profiles locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dac655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import os\n",
    "import rootutils\n",
    "import sys\n",
    "\n",
    "root = rootutils.setup_root(os.path.abspath(\"\"), dotenv=True, pythonpath=True, cwd=False)\n",
    "sys.path.append(os.environ[\"EVENT_STREAM_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23533dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from humanize import naturalsize, naturaldelta\n",
    "from pathlib import Path\n",
    "from sparklines import sparklines\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "from EventStream.data.config import PytorchDatasetConfig\n",
    "from EventStream.data.types import PytorchBatch\n",
    "from EventStream.data.pytorch_dataset import PytorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ae9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "COHORT_NAME = \"MIMIC_IV/ESD_07-23-23_150GB_10cpu-1\"\n",
    "PROJECT_DIR = Path(os.environ[\"PROJECT_DIR\"])\n",
    "dataset_dir = PROJECT_DIR / \"data\" / COHORT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6314e23",
   "metadata": {},
   "source": [
    "First, let's check and see how much disk space the dataset uses, and in what components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3490e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total dataset takes up 65.5 GB on disk, which includes:\n",
      "  * 1.0 GB for the core dataset.\n",
      "  * 2.4 GB for the deep-learning representation dataframes.\n",
      "  * 62.1 GB for the flat representation dataframes.\n"
     ]
    }
   ],
   "source": [
    "total_dataset_size = sum(f.stat().st_size for f in dataset_dir.glob('**/*') if f.is_file())\n",
    "DL_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"DL_reps\").glob('**/*') if f.is_file())\n",
    "just_dataset_size = total_dataset_size - DL_reps_size\n",
    "\n",
    "if (dataset_dir / \"flat_reps\").is_dir():\n",
    "    flat_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"flat_reps\").glob('**/*') if f.is_file())\n",
    "    just_dataset_size -= flat_reps_size\n",
    "    flat_reps_lines = [f\"  * {naturalsize(flat_reps_size)} for the flat representation dataframes.\"]\n",
    "else:\n",
    "    flat_reps_lines = []\n",
    "\n",
    "lines = [\n",
    "    f\"The total dataset takes up {naturalsize(total_dataset_size)} on disk, which includes:\",\n",
    "    f\"  * {naturalsize(just_dataset_size)} for the core dataset.\",\n",
    "    f\"  * {naturalsize(DL_reps_size)} for the deep-learning representation dataframes.\",\n",
    "] + flat_reps_lines\n",
    "\n",
    "print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62d866",
   "metadata": {},
   "source": [
    "First, we'll note that loading a dataset doesn't require much of either resource. This is because the data is loaded lazily, so complex dataframe elements aren't loaded until they are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48bb0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 866.52 MiB, increment: 3.37 MiB\n",
      "CPU times: user 409 ms, sys: 37.3 ms, total: 446 ms\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92467145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /n/data1/hms/dbmi/zaklab/RAMMS/data/MIMIC_IV/ESD_07-23-23_150GB_10cpu-1/subjects_df.parquet...\n",
      "Loading events from /n/data1/hms/dbmi/zaklab/RAMMS/data/MIMIC_IV/ESD_07-23-23_150GB_10cpu-1/events_df.parquet...\n",
      "Loading dynamic_measurements from /n/data1/hms/dbmi/zaklab/RAMMS/data/MIMIC_IV/ESD_07-23-23_150GB_10cpu-1/dynamic_measurements_df.parquet...\n",
      "peak memory: 18050.05 MiB, increment: 17634.09 MiB\n",
      "CPU times: user 26.4 s, sys: 13.9 s, total: 40.2 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "s_df = ESD.subjects_df\n",
    "e_df = ESD.events_df\n",
    "dm_df = ESD.dynamic_measurements_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee9d9d",
   "metadata": {},
   "source": [
    "## Pytorch Dataset Stats\n",
    "Now let's load a pytorch dataset and examine iteration speed and GPU memory cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e8e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(arr: list[float], strify: Callable[float, str] = naturalsize) -> str:\n",
    "    mean, std, mn, mx = np.mean(arr), np.std(arr), np.min(arr), np.max(arr)\n",
    "    simple_summ = f\"{strify(mean)} Â± {strify(std)} ({strify(mn)}-{strify(mx)})\"\n",
    "    \n",
    "    if len(arr) < 25: return simple_summ\n",
    "    \n",
    "    hist_vals, hist_bins = np.histogram(arr)\n",
    "    lines = [simple_summ, \"Histogram:\"]\n",
    "    sparkline = sparklines(hist_vals)\n",
    "    \n",
    "    lines.extend(sparkline)\n",
    "    left_end = strify(hist_bins[0])\n",
    "    right_end = strify(hist_bins[1])\n",
    "    W = len(sparkline[0]) - len(left_end) - len(right_end)\n",
    "    \n",
    "    if W > 0:\n",
    "        lines.append(f\"{left_end}{'-'*W}{right_end}\")\n",
    "    else:\n",
    "        lines.append(f\"o {left_end} (left endpoint)\")\n",
    "        lines.append(f\"{'-'*(len(sparkline[0])-1)}o {right_end} (right endpoint)\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def summarize_times(arr: list[float, timedelta]):\n",
    "    as_seconds = [x / timedelta(seconds=1) for x in arr]\n",
    "    return summarize(as_seconds, strify=lambda x: str(timedelta(seconds=x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b74328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_batch_iteration_speed_and_cost(\n",
    "    batch_size: int,\n",
    "    pyd: Dataset,\n",
    "    n_iter_samples: int = 30,\n",
    "    collate_fn: Callable | None = None,\n",
    "):\n",
    "    def make_dataloader():\n",
    "        if collate_fn is None:\n",
    "            return DataLoader(pyd, batch_size=batch_size, shuffle=True)\n",
    "        return DataLoader(pyd, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dataloader = make_dataloader()\n",
    "    batch_sizes = defaultdict(list)\n",
    "    total_sizes = []\n",
    "    for batch in tqdm(dataloader, leave=False):\n",
    "        total_size = 0\n",
    "        for k, v in batch.items():\n",
    "            if v is None: continue\n",
    "            el_size = v.element_size() * v.nelement()\n",
    "            batch_sizes[k].append(el_size)\n",
    "            total_size += el_size\n",
    "        total_sizes.append(total_size)\n",
    "\n",
    "    batch_iteration_times = []\n",
    "    for samp in tqdm(list(range(n_iter_samples)), leave=False, desc=\"Sampling Dataloader Iteration Speed\"):\n",
    "        dataloader = make_dataloader()\n",
    "        st = datetime.now()\n",
    "        for batch in tqdm(dataloader, leave=False, desc=\"Sampling Batch\"):\n",
    "            pass\n",
    "        batch_iteration_times.append((datetime.now() - st) / len(dataloader))\n",
    "\n",
    "    print(\n",
    "        f\"Iterating through an entire dataloader of {len(dataloader)} batches of size {batch_size} \"\n",
    "        f\"took the following time per batch:\\n{summarize_times(batch_iteration_times)}\\n\\n\"\n",
    "        f\"Total batch size:\\n{summarize(total_sizes)}\"\n",
    "    )\n",
    "    for k, v in batch_sizes.items():\n",
    "        print(f\"  Size of {k}:\\n    {summarize(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d054bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 19293.29 MiB, increment: 7612.77 MiB\n",
      "CPU times: user 57.4 s, sys: 7.93 s, total: 1min 5s\n",
      "Wall time: 42.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "pyd_config = PytorchDatasetConfig(\n",
    "    save_dir=ESD.config.save_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "pyd = PytorchDataset(config=pyd_config, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c71a555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 607 batches of size 16 took the following time per batch:\n",
      "0:00:00.416088 Â± 0:00:00.004160 (0:00:00.410064-0:00:00.422636)\n",
      "\n",
      "Total batch size:\n",
      "67.2 MB Â± 35.0 MB (18.3 MB-410.6 MB)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 18.3 MB (left endpoint)\n",
      "---------o 57.6 MB (right endpoint)\n",
      "  Size of event_mask:\n",
      "    10.9 kB Â± 3.6 kB (3.3 kB-16.4 kB)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 3.3 kB (left endpoint)\n",
      "---------o 4.6 kB (right endpoint)\n",
      "  Size of time_delta:\n",
      "    43.8 kB Â± 14.4 kB (13.1 kB-65.5 kB)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 13.1 kB (left endpoint)\n",
      "---------o 18.3 kB (right endpoint)\n",
      "  Size of static_indices:\n",
      "    127 Bytes Â± 3 Bytes (48 Bytes-128 Bytes)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 48 Bytes (left endpoint)\n",
      "---------o 56 Bytes (right endpoint)\n",
      "  Size of static_measurement_indices:\n",
      "    127 Bytes Â± 3 Bytes (48 Bytes-128 Bytes)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 48 Bytes (left endpoint)\n",
      "---------o 56 Bytes (right endpoint)\n",
      "  Size of dynamic_indices:\n",
      "    25.6 MB Â± 13.3 MB (7.0 MB-156.4 MB)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 7.0 MB (left endpoint)\n",
      "---------o 21.9 MB (right endpoint)\n",
      "  Size of dynamic_measurement_indices:\n",
      "    25.6 MB Â± 13.3 MB (7.0 MB-156.4 MB)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 7.0 MB (left endpoint)\n",
      "---------o 21.9 MB (right endpoint)\n",
      "  Size of dynamic_values:\n",
      "    12.8 MB Â± 6.7 MB (3.5 MB-78.2 MB)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 3.5 MB (left endpoint)\n",
      "---------o 11.0 MB (right endpoint)\n",
      "  Size of dynamic_values_mask:\n",
      "    3.2 MB Â± 1.7 MB (872.4 kB-19.5 MB)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 872.4 kB (left endpoint)\n",
      "---------o 2.7 MB (right endpoint)\n",
      "peak memory: 18728.51 MiB, increment: 993.68 MiB\n",
      "CPU times: user 1h 49min 7s, sys: 13min 6s, total: 2h 2min 13s\n",
      "Wall time: 25min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "profile_batch_iteration_speed_and_cost(batch_size=16, pyd=pyd, n_iter_samples=5, collate_fn=pyd.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd999ce",
   "metadata": {},
   "source": [
    "## Other Pipelines\n",
    "### TemporAI Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a013487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d978222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESD_to_temporai(ESD: Dataset, n_subjects: int | None = 1000) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Converts an ESD data format into a TemporAI dataset format.\"\"\"\n",
    "    \n",
    "    if n_subjects is None:\n",
    "        subject_ids = list(ESD.split_subjects['train'])\n",
    "    else:\n",
    "        subject_ids = list(np.random.default_rng(1).choice(list(ESD.split_subjects['train']), size=n_subjects))\n",
    "\n",
    "    static_df = (\n",
    "        ESD.subjects_df\n",
    "        .filter(pl.col('subject_id').is_in(subject_ids))\n",
    "        .select(\n",
    "            'subject_id',\n",
    "            *[pl.col(c) for c, cfg in ESD.measurement_configs.items() if cfg.temporality == 'static']\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(\"subject_id\")\n",
    "    )\n",
    "    \n",
    "    # For the time-series dataframe, as they need only one row per subject ID, timestamp, we need to use the wide\n",
    "    # format of the flat representation. \n",
    "    \n",
    "    flat_reps_dir = ESD.config.save_dir / \"flat_reps\" / \"raw\"\n",
    "    if not flat_reps_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Must have pre-cached flat representations at {flat_reps_dir}!\")\n",
    "        \n",
    "    ts_dfs = []\n",
    "    for fp in tqdm(list(flat_reps_dir.glob(\"*/*.parquet\")), leave=False, desc=\"Flat Rep Files\"):\n",
    "        new_df = (\n",
    "            pl.scan_parquet(fp)\n",
    "            .filter(pl.col('subject_id').is_in(subject_ids))\n",
    "            .select(\"subject_id\", \"timestamp\", cs.starts_with(\"dynamic\"))\n",
    "            .collect(streaming=True)\n",
    "            .to_pandas()\n",
    "            .set_index([\"subject_id\", \"timestamp\"])\n",
    "        )\n",
    "        \n",
    "        if len(new_df) > 0:\n",
    "            ts_dfs.append(new_df)\n",
    "    \n",
    "    return static_df, ts_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67048d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing chunk size to existing record (250).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4b210f48354fb8a491268f92946305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening Splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 17883.32 MiB, increment: 1.73 MiB\n",
      "CPU times: user 4.47 s, sys: 4.62 s, total: 9.09 s\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# We need to convert to a flat format prior to getting temporai representations.\n",
    "# The performance #s here are not reliable as these files may be already generated.\n",
    "ESD.cache_flat_representation(\n",
    "    subjects_per_output_file=None,\n",
    "    feature_inclusion_frequency=None,\n",
    "    do_overwrite=False,\n",
    "    do_update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bde3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /n/data1/hms/dbmi/zaklab/RAMMS/data/MIMIC_IV/ESD_07-23-23_150GB_10cpu-1/subjects_df.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flat Rep Files:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 7941.90 MiB, increment: 7538.58 MiB\n",
      "CPU times: user 5min 45s, sys: 1min 44s, total: 7min 30s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# This is for subsampling to make the full dataset less expensive to load.\n",
    "# Even subsampling down to 1000 patients makes the temporai fully padded files\n",
    "# too expensive to store in 75GB of memory.\n",
    "n_subjects = 250\n",
    "temporai_subsample_rate = n_subjects / len(ESD.split_subjects['train'])\n",
    "temporai_static, temporai_ts_L = ESD_to_temporai(ESD, n_subjects=n_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b886c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 10914.00 MiB, increment: 4157.78 MiB\n",
      "CPU times: user 7.63 s, sys: 3.33 s, total: 11 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "temporai_ts = pd.concat(temporai_ts_L, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f6cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporAI uses two dataframes, a static dataframe of shape (249, 1) and a time series dataframe of shape (51954, 13761). This is sub-sampled at a rate of  2.1%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"TemporAI uses two dataframes, a static dataframe of shape {temporai_static.shape} \"\n",
    "    f\"and a time series dataframe of shape {temporai_ts.shape}. \"\n",
    "    f\"This is sub-sampled at a rate of {(temporai_subsample_rate) * 100 : .1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7346bbe",
   "metadata": {},
   "source": [
    "Let's save these dataframes to disk, so we can inspect their disk cost and the memory cost to re-load them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35b8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"./speed_comparisons/temporai/compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41127e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compressed data takes up 62.3 MB on disk.\n",
      "The uncompressed data takes up 70.2 MB on disk (this is a good approximation of memory cost as it is uncompressed).\n",
      "Scaling for the subsampling to only 250 patients, this would equate to approximately: The compressed data takes up 3.0 GB on disk.\n",
      "The uncompressed data takes up 3.4 GB on disk (this is a good approximation of memory cost as it is uncompressed).\n",
      "peak memory: 11097.99 MiB, increment: 212.54 MiB\n",
      "CPU times: user 32.3 s, sys: 1.73 s, total: 34.1 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts.to_parquet(save_dir / \"ts.parquet\")\n",
    "\n",
    "uncompressed_save_dir = Path(\"./speed_comparisons/temporai/uncompressed\")\n",
    "uncompressed_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(uncompressed_save_dir / \"static.parquet\", compression=None)\n",
    "temporai_ts.to_parquet(uncompressed_save_dir / \"ts.parquet\", compression=None)\n",
    "\n",
    "compressed_temporai_size = sum(f.stat().st_size for f in save_dir.glob('**/*') if f.is_file())\n",
    "uncompressed_temporai_size = sum(f.stat().st_size for f in uncompressed_save_dir.glob('**/*') if f.is_file())\n",
    "\n",
    "print(\n",
    "    f\"The compressed data takes up {naturalsize(compressed_temporai_size)} on disk.\\n\"\n",
    "    f\"The uncompressed data takes up {naturalsize(uncompressed_temporai_size)} on disk \"\n",
    "    \"(this is a good approximation of memory cost as it is uncompressed).\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Scaling for the subsampling to only {n_subjects} patients, this would equate to approximately:\\n\"\n",
    "    f\"The compressed data takes up {naturalsize(compressed_temporai_size/subsample_rate, format='%.3f')} \"\n",
    "    \" on disk.\\n\"\n",
    "    f\"The uncompressed data takes up {naturalsize(uncompressed_temporai_size/subsample_rate, format='%.3f')} \"\n",
    "    \"on disk (this is a good approximation of memory cost).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a5fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 9078.91 MiB, increment: 8683.60 MiB\n",
      "CPU times: user 7.64 s, sys: 5.55 s, total: 13.2 s\n",
      "Wall time: 4.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "temporai_static = pd.read_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts = pd.read_parquet(save_dir / \"ts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e20635",
   "metadata": {},
   "source": [
    "Recall that this is under the effect of sub-sampling, so a true memory cost of loading all the data in this format would have a peak of approximately 8719.1 * (100/2.1) = 415 GB!\n",
    "\n",
    "TemporAI generally converts their timeseries data into a dense, 3D matrix across samples, timepoints, and features. For use in ML pipelines, this is then generally iterated through directly via simple numpy iteration. \n",
    "\n",
    "For example: \n",
    "  * Datasets are converted to 3D views here: https://github.com/vanderschaarlab/temporai/blob/main/src/tempor/plugins/prediction/one_off/classification/__init__.py#L59 and https://github.com/vanderschaarlab/temporai/blob/67ebd74dc24728163d9aec37f1771a83fc3346e2/src/tempor/data/utils.py#L49\n",
    "  * Iteration through numpy arrays happens here: https://github.com/vanderschaarlab/temporai/blob/main/src/tempor/models/ddh.py#L155\n",
    "  \n",
    "Though a full comparison warrants use of their library (and will further depend on the exact model used (as each has different strategies for processing data), we can simulate that approach here quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef8c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_categories(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df[c]):\n",
    "            df[c] = df[c].cat.codes\n",
    "    return df\n",
    "\n",
    "def to_3D_arr(df: pd.DataFrame, max_timesteps: int | None = None) -> np.ndarray:\n",
    "    df = no_categories(df)\n",
    "    samples = set(df.index.get_level_values(0))\n",
    "    num_samples = len(samples)\n",
    "    num_features = len(df.columns)\n",
    "    num_timesteps_per_sample = df.groupby(level=0).size()\n",
    "    max_actual_timesteps = num_timesteps_per_sample.max()\n",
    "    max_timesteps = max_actual_timesteps if max_timesteps is None else max_timesteps\n",
    "    array = np.full(shape=(num_samples, max_timesteps, num_features), fill_value=np.NaN)\n",
    "    for i_sample, idx_sample in enumerate(samples):\n",
    "        set_vals = df.loc[idx_sample, :, :].to_numpy()[:max_timesteps, :]  # pyright: ignore\n",
    "        if i_sample == 0:\n",
    "            array = array.astype(set_vals.dtype)  # Need to cast to the type matching source data.\n",
    "        array[i_sample, : num_timesteps_per_sample[idx_sample], :] = set_vals  # pyright: ignore\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f111193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTemporAIStyleDataset(Dataset):\n",
    "    def __init__(self, static: np.ndarray, ts: np.ndarray):\n",
    "        self.static = static\n",
    "        self.ts = ts\n",
    "        \n",
    "    def __len__(self) -> int: return self.ts.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict[str, torch.Tensor]:\n",
    "        return {'static': torch.Tensor(self.static[idx]), 'ts': torch.Tensor(self.ts[idx])}\n",
    "    \n",
    "def profile_temporai_dataset(\n",
    "    temporai_static, temporai_ts, batch_size: int = 16,\n",
    "    n_iter_samples: int = 30,\n",
    "    max_seq_len: int = 32,\n",
    "):\n",
    "    static_as_np = np.nan_to_num(no_categories(temporai_static).to_numpy(), nan=0)\n",
    "    ts_as_np = np.nan_to_num(to_3D_arr(temporai_ts, max_timesteps=max_seq_len), nan=0)\n",
    "    print(\n",
    "        f\"Yielded a static NP array of shape {static_as_np.shape} and a TS NP array \"\n",
    "        f\"of shape {ts_as_np.shape}.\"\n",
    "    )\n",
    "    temporai_pyd = SimpleTemporAIStyleDataset(static_as_np, ts_as_np)\n",
    "\n",
    "    profile_batch_iteration_speed_and_cost(\n",
    "        batch_size=batch_size, pyd=temporai_pyd, n_iter_samples=n_iter_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielded a static NP array of shape (249, 1) and a TS NP array of shape (249, 1024, 13761).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 16 batches of size 16 took the following time per batch:\n",
      "0:00:00.596937 Â± 0:00:00.020572 (0:00:00.571113-0:00:00.623727)\n",
      "\n",
      "Total batch size:\n",
      "877.2 MB Â± 95.5 MB (507.3 MB-901.8 MB)\n",
      "  Size of static:\n",
      "    62 Bytes Â± 6 Bytes (36 Bytes-64 Bytes)\n",
      "  Size of ts:\n",
      "    877.2 MB Â± 95.5 MB (507.3 MB-901.8 MB)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "profile_temporai_dataset(\n",
    "    temporai_static, temporai_ts, batch_size=16, n_iter_samples=5, max_seq_len=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24156f",
   "metadata": {},
   "source": [
    "As we can see, the strategy of featurizing and batching used in TemporAI results (on this synthetic dataset) in a significantly faster iteration speed and a marginally lower memory cost than does the strategy used in ESGPT (all formats are mean Â± standard deviation (min - max)\n",
    "\n",
    "TemporAI Speed: `0:00:00.596937 Â± 0:00:00.020572 (0:00:00.571113-0:00:00.623727)`  \n",
    "ESGPT Speed:    `0:00:00.416088 Â± 0:00:00.004160 (0:00:00.410064-0:00:00.422636)`\n",
    "\n",
    "TemporAI Memory: `877.2 MB Â± 95.5 MB (507.3 MB-901.8 MB)`  \n",
    "ESGPT Memory:    `67.2 MB Â± 35.0 MB (18.3 MB-410.6 MB)`\n",
    "\n",
    "In table form (using chatGPT for conversions, so may need to be double checked), where \"Delta\" means what % of TemporAI's resource cost does ESGPT _save_ (higher is better), we get the following:\n",
    "|                      | TemporAI             | ESGPT                | Improvement (%)  |\n",
    "|----------------------|----------------------|----------------------|-------------------|\n",
    "| **Speed (ms)**       | 597 Â± 20.6 (571 - 624) | 416 Â± 4.16 (410 - 423) | 30.3%            |\n",
    "| **Memory (MB)**      | 877 Â± 95.5 (507 - 902) | 67.2 Â± 35.0 (18.3 - 411) | 92.3%           |\n",
    "\n",
    "There are some biases in this format, on both sides:\n",
    "  1. ESGPT samples different subsequences per item iteration, whereas TemporAI is limited to only using the first max subsequence samples. \n",
    "  2. This dataset has relatively few measurements, which will reduce the memory disparity between the two formats (this bias favors TemporAI).\n",
    "  3. The strategy of flattening this dataset may induce too much memory overhead, as if multiple measurements are not common within an event, it will have extra columns that TemporAI does not need. Conversely, it may reduce a significant amount of data, as if there are many measurements than a simple count, sum, sum_sqd, min, and max representation will not fully capture the data, thereby reducing the burden on TemporAI. (This bias could favor either).\n",
    "  4. This example uses a long max sequence length, which opens up more opportunities for ESGPT's dynamic padding scheme to save memory. (This bias favors ESGPT on Memory).\n",
    "  5. This example uses a long max sequence length, which means that ESGPT's per-event measurement padding workflow will take longer per batch (This bias favors TemporAI on speed).\n",
    "  \n",
    "### omop-learn\n",
    "\n",
    "Next, we'll attempt to compare against [omop-Learn](https://github.com/clinicalml/omop-learn/tree/master). omop-learn uses a similar storage model to ESGPT (a _long_ format) but it does not include the storage of numerical values, only codes, and it similarly does not allow for randomly sampled sub-sequences per dataset item. See [here](https://github.com/clinicalml/omop-learn/blob/a33440af2b9f1342e0c16106acf93131b9369441/src/omop_learn/torch/data.py) for more details. To simulate this, we'll first convert the ESGPT data into a pre-tokenized OMOP-learn representation, store in the desired format (JSON), then re-load it and iterate through it in the same manner as omop-learn does. As static data are not separately encoded in the omop-learn dataset format, we'll omit that as well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4edcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json, shutil, pandas as pd, polars as pl\n",
    "REFTIME = pd.Timestamp(\"2000-01-01\")\n",
    "\n",
    "def to_unixtime(str_time_array):\n",
    "    unix_times = (pd.to_datetime(str_time_array) - REFTIME) // pd.Timedelta(\"1d\")\n",
    "    return unix_times\n",
    "\n",
    "\n",
    "def from_unixtime(unix_time_array):\n",
    "    datetimes = [pd.to_datetime(t * pd.Timedelta(\"1d\") + REFTIME) for t in unix_time_array]\n",
    "    return datetimes\n",
    "\n",
    "def ESD_row_to_omoplearn_row(row: dict[str, Any], unified_vocab: dict[int, str]) -> dict[str, Any]:\n",
    "    example = {}\n",
    "    \n",
    "    # Dates\n",
    "    example['dates'] = [str(row['start_time'] + timedelta(minutes=t)) for t in row['time']]\n",
    "    \n",
    "    # Visits\n",
    "    example['visits'] = [\n",
    "        [unified_vocab[idx] for idx in indices] for indices in row['dynamic_indices']\n",
    "    ]\n",
    "    \n",
    "    example['tok_visits'] = row['dynamic_indices']\n",
    "    \n",
    "    assert len(example['dates']) == len(example['visits'])\n",
    "    assert len(example['dates']) == len(example['tok_visits'])\n",
    "    \n",
    "    example['y'] = 0\n",
    "    \n",
    "    return example\n",
    "\n",
    "def ESD_to_omoplearn(ESD: Dataset, n_subjects: int | None) -> list[dict[str, Any]]:\n",
    "    if n_subjects is None:\n",
    "        subject_ids = list(ESD.split_subjects['train'])\n",
    "    else:\n",
    "        subject_ids = list(np.random.default_rng(1).choice(list(ESD.split_subjects['train']), size=n_subjects))\n",
    "    \n",
    "    unified_vocab = {}\n",
    "    for m, idxmap in ESD.unified_vocabulary_idxmap.items():\n",
    "        for k, v in idxmap.items():\n",
    "            unified_vocab[v] = f\"{m}/{k}\"\n",
    "    \n",
    "    omop_file_dir = Path(\"./omop_reps\")\n",
    "    if omop_file_dir.is_dir():\n",
    "        shutil.rmtree(omop_file_dir)\n",
    "\n",
    "    omop_file_dir.mkdir(exist_ok=True, parents=True)\n",
    "    sp = \"train\"\n",
    "    \n",
    "    omop_sp_dir = omop_file_dir / sp\n",
    "    omop_sp_dir.mkdir(exist_ok=True, parents=True)\n",
    "    for fp in tqdm(\n",
    "        list((ESD.config.save_dir / \"DL_reps\").glob(f\"{sp}_*.parquet\")), desc=\"File\", leave=False\n",
    "    ):\n",
    "        DL_reps_df = (\n",
    "            pl.scan_parquet(fp)\n",
    "            .filter(pl.col('subject_id').is_in(subject_ids))\n",
    "            .select('subject_id', 'start_time', 'time', 'dynamic_indices')\n",
    "            .collect()\n",
    "        )\n",
    "        cols = DL_reps_df.columns\n",
    "        rows = DL_reps_df.rows()\n",
    "\n",
    "        for row in rows:\n",
    "            row_as_dict = {col: val for col, val in zip(cols, row)}\n",
    "            example = ESD_row_to_omoplearn_row(row_as_dict, unified_vocab)\n",
    "            with open(omop_sp_dir / \"data.json\", \"a\") as f:\n",
    "                json.dump(example, f)\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da85f85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "File:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 631.82 MiB, increment: 228.32 MiB\n",
      "CPU times: user 10.4 s, sys: 673 ms, total: 11.1 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "n_subjects = 250\n",
    "omop_subsample_rate = n_subjects / len(ESD.split_subjects['train'])\n",
    "ESD_to_omoplearn(ESD, n_subjects=n_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96350313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON dumped dataset takes up 69.6 MB on disk.\n",
      "Given subsampling, this equates to approximately 2.700 GB.\n"
     ]
    }
   ],
   "source": [
    "JSON_dataset_size = sum(f.stat().st_size for f in Path(\"./omop_reps\").glob('**/*') if f.is_file())\n",
    "print(\n",
    "    f\"The JSON dumped dataset takes up {naturalsize(JSON_dataset_size)} on disk.\\n\"\n",
    "    \"Given subsampling, this equates to approximately \"\n",
    "    f\"{naturalsize(JSON_dataset_size/omop_subsample_rate, format='%.3f')}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01aaa861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightly adapted from https://github.com/clinicalml/omop-learn/blob/a33440af2b9f1342e0c16106acf93131b9369441/src/omop_learn/torch/data.py\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class OMOPDatasetTorch(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        omop_dataset_file,\n",
    "        max_num_visits=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.items = {}\n",
    "        self.visit_sequences = []  # patient x (# visits for patient) lists w/ concepts expressed\n",
    "        self.time_sequences = []  # patient x (# visits for patient) times of visits\n",
    "        self.visit_sizes = []  # patient x (# visits for patient) # concepts in each visit\n",
    "        self.outcomes = []  # patient--outcome for each patient\n",
    "        self.tok_visit_sequences = None\n",
    "        self.tokenizer = None\n",
    "        self.max_num_visits = max_num_visits  # if set, truncate to most recent\n",
    "        self._load_json(omop_dataset_file, False)\n",
    "\n",
    "\n",
    "    def _load_json(self, path_to_json, tokenize_on_load):\n",
    "        # read once to build concept set\n",
    "        # (and load visits if tokenize_on_load=False)\n",
    "        concept_set = set()\n",
    "        concept_counts = Counter()\n",
    "        concept_counts_by_year = Counter()\n",
    "        years = set()\n",
    "        max_num_visits = 0\n",
    "        skipped = 0\n",
    "        with open(path_to_json) as json_fh:\n",
    "            for i, line in enumerate(json_fh.readlines()):\n",
    "                example = self._process_line(line)\n",
    "                max_num_visits = max(max_num_visits, len(example['visits']))\n",
    "\n",
    "                for time, visit in zip(example['unix_times'], example['visits']):\n",
    "                    for concept in visit:\n",
    "                        concept_set.add(concept)\n",
    "\n",
    "                if len(example['visits']) == 0:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                if i == 0:\n",
    "                    for key, value in example.items():\n",
    "                        self.items[key] = [value]\n",
    "                else:\n",
    "                    # correctly gives error when key is not found\n",
    "                    # already; all items need to have exactly the same\n",
    "                    # set of keys.\n",
    "                    for key,value in example.items():\n",
    "                        self.items[key].append(value)\n",
    "\n",
    "        print(f\"Skipped {skipped} patients for empty visit lists\")\n",
    "        if not self.max_num_visits:\n",
    "            self.max_num_visits = max_num_visits\n",
    "\n",
    "#         if not self.tokenizer:\n",
    "#             self.tokenizer = ConceptTokenizer(concept_set)\n",
    "#             print(\"built tokenizer\")\n",
    "\n",
    "#         # read again to build tokenized visits\n",
    "#         if tokenize_on_load:\n",
    "#             self.items['tok_visits'] = []\n",
    "#             with open(path_to_json, \"r\") as json_fh:\n",
    "#                 for i, line in enumerate(json_fh.readlines()):\n",
    "#                     example = self._process_line(line)\n",
    "#                     tok_visit_list = []\n",
    "#                     for visit in example['visits']:\n",
    "#                         tok_visit = self.tokenizer.concepts_to_ids(visit)\n",
    "#                         tok_visit_list.append(tok_visit)\n",
    "#                     if len(tok_visit_list) > 0:\n",
    "#                         self.items['tok_visits'].append(tok_visit_list)\n",
    "\n",
    "        self.outcomes = torch.LongTensor(self.items['y'])\n",
    "        self.one_fraction = self.outcomes.sum() / len(self.outcomes)\n",
    "        self.one_odds = self.one_fraction / (1 - self.one_fraction)\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        example = json.loads(line)\n",
    "        dates = example['dates']\n",
    "        unix_times = to_unixtime(dates)\n",
    "        example['unix_times'] = unix_times\n",
    "\n",
    "        # make sure visits are sorted by date\n",
    "        # This actually contains a minor error correction in omop-learn; namely, that pre-tokenized visits may not\n",
    "        # have been properly sorted.\n",
    "        sorted_visits = [v for d,v in sorted(zip(example['unix_times'], example['visits']))]\n",
    "        if 'tok_visits' in example:\n",
    "            sorted_tok_visits = [t for d,t in sorted(zip(example['unix_times'], example['tok_visits']))]\n",
    "            example['tok_visits'] = sorted_tok_visits\n",
    "        example['visits'] = sorted_visits\n",
    "        example['unix_times'] = sorted(example['unix_times'])\n",
    "        example['dates'] = sorted(example['dates'])\n",
    "        \n",
    "        assert len(example['visits']) == len(example['unix_times'])\n",
    "        assert len(example['tok_visits']) == len(example['unix_times'])\n",
    "\n",
    "        return example\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = {k : v[idx] for k,v in self.items.items()}\n",
    "        times = torch.LongTensor(example['unix_times'])\n",
    "        visits = example['visits'] if 'tok_visits' not in example else example['tok_visits']\n",
    "        assert len(times) == len(visits)\n",
    "\n",
    "        # trim before tokenizing\n",
    "        visits = visits[-self.max_num_visits :]\n",
    "        times = times[-self.max_num_visits :]\n",
    "\n",
    "        if 'tok_visits' not in example:\n",
    "            raise NotImplementedError(f\"Must be pre-tokenized!\")\n",
    "#             tok_visits = []\n",
    "#             for visit in example['visits']:\n",
    "#                 tok_visits.append(self.tokenizer.concepts_to_ids(visit))\n",
    "#             visits = tok_visits\n",
    "\n",
    "        example['visits'] = visits\n",
    "        # Another small bug correction\n",
    "        example['unix_times'] = times\n",
    "\n",
    "        visit_sizes = torch.LongTensor([len(v) for v in visits])\n",
    "        outcome = self.outcomes[idx]\n",
    "        nvisits = len(visits)\n",
    "\n",
    "        return example\n",
    "\n",
    "    # pads a batch to largest # of visits / patient in the batch\n",
    "    # and largest # of concepts / visit along concept dim.\n",
    "    def collate(self, batch):\n",
    "        # first group along dict keys\n",
    "        batch_collated = {}\n",
    "        for k in batch[0].keys():\n",
    "            batch_collated[k] = [b[k] for b in batch]\n",
    "\n",
    "        keys = list(batch_collated.keys())\n",
    "        N = len(batch_collated['y'])\n",
    "\n",
    "        # each patient is a list of visits\n",
    "        max_num_visits = max([len(v) for v in batch_collated['visits']])\n",
    "        max_num_concepts = max(l for p in range(N) for l in [len(v) for v in batch_collated['visits'][p]])\n",
    "\n",
    "        concept_tensor = torch.full(\n",
    "            (N, max_num_visits, max_num_concepts),\n",
    "            0,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "        times_tensor = torch.full((N, max_num_visits), -1, dtype=torch.long)\n",
    "        batch = batch_collated\n",
    "\n",
    "        lengths = torch.zeros(N)\n",
    "        for i, visit_list in enumerate(batch['visits']):\n",
    "            assert len(visit_list) == len(batch['unix_times'][i]), (\n",
    "                f\"Visits don't match! Got {len(visit_list)} visits and {len(batch['unix_times'][i])} times \"\n",
    "                f\"for batch element {i}\"\n",
    "            )\n",
    "            num_visits = len(visit_list)  # visits of this patient we are including\n",
    "            lengths[i] = num_visits\n",
    "            for j, visit in enumerate(visit_list):\n",
    "                visit_size = len(batch['visits'][i][j])\n",
    "                assert(visit_size == len(visit))\n",
    "                concept_tensor[i, j, : visit_size] = torch.Tensor(visit)\n",
    "            times_tensor[i, :num_visits] = torch.Tensor(batch['unix_times'][i])\n",
    "        batch.pop(\"unix_times\")\n",
    "        batch.pop(\"dates\")\n",
    "        # Another small correction:\n",
    "        batch.pop(\"tok_visits\")\n",
    "        batch[\"visits\"] = concept_tensor\n",
    "        batch[\"times\"] = times_tensor\n",
    "        batch[\"lengths\"] = lengths\n",
    "        for k,v in batch.items():\n",
    "            if not isinstance(v, torch.Tensor):\n",
    "                batch[k] = torch.tensor(v)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41df454",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 patients for empty visit lists\n",
      "peak memory: 830.48 MiB, increment: 314.32 MiB\n",
      "CPU times: user 1.47 s, sys: 318 ms, total: 1.78 s\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "ODT = OMOPDatasetTorch(\"./omop_reps/train/data.json\", max_num_visits=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5b9596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 16 batches of size 16 took the following time per batch:\n",
      "0:00:00.174998 Â± 0:00:00.016615 (0:00:00.142167-0:00:00.186295)\n",
      "\n",
      "Total batch size:\n",
      "25.3 MB Â± 10.7 MB (9.5 MB-46.1 MB)\n",
      "  Size of visits:\n",
      "    25.3 MB Â± 10.7 MB (9.5 MB-46.0 MB)\n",
      "  Size of y:\n",
      "    123 Bytes Â± 17 Bytes (56 Bytes-128 Bytes)\n",
      "  Size of times:\n",
      "    83.0 kB Â± 34.8 kB (33.5 kB-131.1 kB)\n",
      "  Size of lengths:\n",
      "    61 Bytes Â± 8 Bytes (28 Bytes-64 Bytes)\n",
      "peak memory: 1103.59 MiB, increment: 184.39 MiB\n",
      "CPU times: user 36 s, sys: 5.75 s, total: 41.7 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "profile_batch_iteration_speed_and_cost(\n",
    "    batch_size=16, pyd=ODT, n_iter_samples=5, collate_fn=ODT.collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf530d",
   "metadata": {},
   "source": [
    "This, as we can see, is faster and smaller than either of the other two alternates. However, recall that this is not a fair comparison; this system ignores continuous values, event masks, and static variables. We can try to assess the impact of these by altering the omop-learn format to include such data, which we'll explore below:\n",
    "\n",
    "### Altered omop-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25c1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESD_row_to_altered_omoplearn_row(row: dict[str, Any], unified_vocab: dict[int, str]) -> dict[str, Any]:\n",
    "    example = {}\n",
    "    \n",
    "    # Dates\n",
    "    example['dates'] = [str(row['start_time'] + timedelta(minutes=t)) for t in row['time']]\n",
    "    \n",
    "    # Visits\n",
    "    example['visits'] = [\n",
    "        [unified_vocab[idx] for idx in indices] for indices in row['dynamic_indices']\n",
    "    ]\n",
    "    \n",
    "    example['tok_visits'] = row['dynamic_indices']\n",
    "    \n",
    "    assert len(example['dates']) == len(example['visits'])\n",
    "    assert len(example['dates']) == len(example['tok_visits'])\n",
    "    \n",
    "    example['visit_nums'] = row['dynamic_values']\n",
    "    assert len(example['dates']) == len(example['visit_nums'])\n",
    "    for v, vn in zip(example['visits'], example['visit_nums']):\n",
    "        assert len(v) == len(vn), f\"Misaligned in conversion! {len(v)} v. {len(vn)}.\"\n",
    "\n",
    "    example['static'] = row['static_indices']\n",
    "    \n",
    "    example['y'] = 0\n",
    "    \n",
    "    return example\n",
    "\n",
    "def ESD_to_altered_omoplearn(ESD: Dataset, n_subjects: int | None) -> list[dict[str, Any]]:\n",
    "    if n_subjects is None:\n",
    "        subject_ids = list(ESD.split_subjects['train'])\n",
    "    else:\n",
    "        subject_ids = list(np.random.default_rng(1).choice(list(ESD.split_subjects['train']), size=n_subjects))\n",
    "    \n",
    "    unified_vocab = {}\n",
    "    for m, idxmap in ESD.unified_vocabulary_idxmap.items():\n",
    "        for k, v in idxmap.items():\n",
    "            unified_vocab[v] = f\"{m}/{k}\"\n",
    "    \n",
    "    omop_file_dir = Path(\"./altered_omop_reps\")\n",
    "    if omop_file_dir.is_dir():\n",
    "        shutil.rmtree(omop_file_dir)\n",
    "\n",
    "    omop_file_dir.mkdir(exist_ok=True, parents=True)\n",
    "    sp = \"train\"\n",
    "    \n",
    "    omop_sp_dir = omop_file_dir / sp\n",
    "    omop_sp_dir.mkdir(exist_ok=True, parents=True)\n",
    "    for fp in tqdm(\n",
    "        list((ESD.config.save_dir / \"DL_reps\").glob(f\"{sp}_*.parquet\")), desc=\"File\", leave=False\n",
    "    ):\n",
    "        DL_reps_df = (\n",
    "            pl.scan_parquet(fp)\n",
    "            .filter(pl.col('subject_id').is_in(subject_ids))\n",
    "            .select('subject_id', 'start_time', 'time', 'dynamic_indices', 'dynamic_values', 'static_indices')\n",
    "            .collect()\n",
    "        )\n",
    "        cols = DL_reps_df.columns\n",
    "        rows = DL_reps_df.rows()\n",
    "\n",
    "        for row in rows:\n",
    "            row_as_dict = {col: val for col, val in zip(cols, row)}\n",
    "            example = ESD_row_to_altered_omoplearn_row(row_as_dict, unified_vocab)\n",
    "            with open(omop_sp_dir / \"data.json\", \"a\") as f:\n",
    "                for v, vn in zip(example['visits'], example['visit_nums']):\n",
    "                    assert len(v) == len(vn), f\"Misaligned in conversion! {len(v)} v. {len(vn)}.\"\n",
    "                json_str = json.dumps(example)\n",
    "                f.write(f\"{json_str}\\n\")\n",
    "                reloaded = json.loads(json_str)\n",
    "                for i, (v, vn) in enumerate(zip(reloaded['visits'], reloaded['visit_nums'])):\n",
    "                    if len(v) != len(vn):\n",
    "                        print(example['visit_nums'][i])\n",
    "                        print(example['visits'][i])\n",
    "                        raise ValueError(f\"Misaligned in conversion! {len(v)} v. {len(vn)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "626255ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "File:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1383.14 MiB, increment: 415.16 MiB\n",
      "CPU times: user 20.2 s, sys: 1.89 s, total: 22.1 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "n_subjects = 250\n",
    "omop_subsample_rate = n_subjects / len(ESD.split_subjects['train'])\n",
    "ESD_to_altered_omoplearn(ESD, n_subjects=n_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d67f2198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025767882910740055\n"
     ]
    }
   ],
   "source": [
    "omop_subsample_rate = 250 / len(ESD.split_subjects['train'])\n",
    "print(omop_subsample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28d477dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON dumped dataset takes up 107.6 MB on disk.\n",
      "Given subsampling, this equates to approximately 4.175 GB.\n"
     ]
    }
   ],
   "source": [
    "JSON_dataset_size = sum(f.stat().st_size for f in Path(\"./altered_omop_reps\").glob('**/*') if f.is_file())\n",
    "print(\n",
    "    f\"The JSON dumped dataset takes up {naturalsize(JSON_dataset_size)} on disk.\\n\"\n",
    "    \"Given subsampling, this equates to approximately \"\n",
    "    f\"{naturalsize(JSON_dataset_size/omop_subsample_rate, format='%.3f')}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8686f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlteredOMOPDatasetTorch(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        omop_dataset_file,\n",
    "        max_num_visits=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.items = {}\n",
    "        self.visit_sequences = []  # patient x (# visits for patient) lists w/ concepts expressed\n",
    "        self.visit_nums_sequences = []\n",
    "        self.static_sequences = []\n",
    "        self.time_sequences = []  # patient x (# visits for patient) times of visits\n",
    "        self.visit_sizes = []  # patient x (# visits for patient) # concepts in each visit\n",
    "        self.outcomes = []  # patient--outcome for each patient\n",
    "        self.tok_visit_sequences = None\n",
    "        self.tokenizer = None\n",
    "        self.max_num_visits = max_num_visits  # if set, truncate to most recent\n",
    "        self._load_json(omop_dataset_file, False)\n",
    "\n",
    "\n",
    "    def _load_json(self, path_to_json, tokenize_on_load):\n",
    "        # read once to build concept set\n",
    "        # (and load visits if tokenize_on_load=False)\n",
    "        concept_set = set()\n",
    "        concept_counts = Counter()\n",
    "        concept_counts_by_year = Counter()\n",
    "        years = set()\n",
    "        max_num_visits = 0\n",
    "        skipped = 0\n",
    "        with open(path_to_json) as json_fh:\n",
    "            for i, line in enumerate(json_fh.readlines()):\n",
    "                example = self._process_line(line)\n",
    "                max_num_visits = max(max_num_visits, len(example['visits']))\n",
    "\n",
    "                for time, visit in zip(example['unix_times'], example['visits']):\n",
    "                    for concept in visit:\n",
    "                        concept_set.add(concept)\n",
    "\n",
    "                if len(example['visits']) == 0:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                if i == 0:\n",
    "                    for key, value in example.items():\n",
    "                        self.items[key] = [value]\n",
    "                else:\n",
    "                    # correctly gives error when key is not found\n",
    "                    # already; all items need to have exactly the same\n",
    "                    # set of keys.\n",
    "                    for key,value in example.items():\n",
    "                        self.items[key].append(value)\n",
    "\n",
    "        print(f\"Skipped {skipped} patients for empty visit lists\")\n",
    "        if not self.max_num_visits:\n",
    "            self.max_num_visits = max_num_visits\n",
    "\n",
    "#         if not self.tokenizer:\n",
    "#             self.tokenizer = ConceptTokenizer(concept_set)\n",
    "#             print(\"built tokenizer\")\n",
    "\n",
    "#         # read again to build tokenized visits\n",
    "#         if tokenize_on_load:\n",
    "#             self.items['tok_visits'] = []\n",
    "#             with open(path_to_json, \"r\") as json_fh:\n",
    "#                 for i, line in enumerate(json_fh.readlines()):\n",
    "#                     example = self._process_line(line)\n",
    "#                     tok_visit_list = []\n",
    "#                     for visit in example['visits']:\n",
    "#                         tok_visit = self.tokenizer.concepts_to_ids(visit)\n",
    "#                         tok_visit_list.append(tok_visit)\n",
    "#                     if len(tok_visit_list) > 0:\n",
    "#                         self.items['tok_visits'].append(tok_visit_list)\n",
    "\n",
    "        self.outcomes = torch.LongTensor(self.items['y'])\n",
    "        self.one_fraction = self.outcomes.sum() / len(self.outcomes)\n",
    "        self.one_odds = self.one_fraction / (1 - self.one_fraction)\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        example = json.loads(line)\n",
    "        dates = example['dates']\n",
    "        unix_times = to_unixtime(dates)\n",
    "        example['unix_times'] = unix_times\n",
    "\n",
    "        # make sure visits are sorted by date\n",
    "        # This actually contains a minor error correction in omop-learn; namely, that pre-tokenized visits may not\n",
    "        # have been properly sorted.\n",
    "#         sorted_visits = [v for d,v in sorted(zip(example['unix_times'], example['visits']))]\n",
    "#         example['visits'] = sorted_visits\n",
    "#         if 'tok_visits' in example:\n",
    "#             sorted_tok_visits = [t for d,t in sorted(zip(example['unix_times'], example['tok_visits']))]\n",
    "#             example['tok_visits'] = sorted_tok_visits\n",
    "#         if 'visit_nums' in example:\n",
    "#             sorted_visit_nums = [t for d,t in sorted(zip(example['unix_times'], example['visit_nums']))]\n",
    "#             example['visit_nums'] = sorted_visit_nums\n",
    "#             assert len(example['visit_nums']) == len(example['visits'])\n",
    "#             for v, vn in zip(example['visits'], example['visit_nums']):\n",
    "#                 assert len(v) == len(vn), f\"Error in parsing! {len(v)} vs. {len(vn)}.\"\n",
    "            \n",
    "#         example['unix_times'] = sorted(example['unix_times'])\n",
    "#         example['dates'] = sorted(example['dates'])\n",
    "        \n",
    "#         assert len(example['visits']) == len(example['unix_times'])\n",
    "#         assert len(example['tok_visits']) == len(example['unix_times'])\n",
    "\n",
    "        return example\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = {k : v[idx] for k,v in self.items.items()}\n",
    "        times = torch.LongTensor(example['unix_times'])\n",
    "        visits = example['visits'] if 'tok_visits' not in example else example['tok_visits']\n",
    "        assert len(times) == len(visits)\n",
    "        assert len(times) == len(example['visit_nums'])\n",
    "        \n",
    "        for v, vn in zip(visits, example['visit_nums']):\n",
    "            assert len(v) == len(vn), f\"Error! {len(v)} vs. {len(vn)}.\"\n",
    "\n",
    "        # trim before tokenizing\n",
    "        visits = visits[-self.max_num_visits :]\n",
    "        times = times[-self.max_num_visits :]\n",
    "        example['visit_nums'] = example['visit_nums'][-self.max_num_visits:]\n",
    "\n",
    "        if 'tok_visits' not in example:\n",
    "            raise NotImplementedError(f\"Must be pre-tokenized!\")\n",
    "#             tok_visits = []\n",
    "#             for visit in example['visits']:\n",
    "#                 tok_visits.append(self.tokenizer.concepts_to_ids(visit))\n",
    "#             visits = tok_visits\n",
    "\n",
    "        example['visits'] = visits\n",
    "        # Another small bug correction\n",
    "        example['unix_times'] = times\n",
    "\n",
    "        visit_sizes = torch.LongTensor([len(v) for v in visits])\n",
    "        outcome = self.outcomes[idx]\n",
    "        nvisits = len(visits)\n",
    "\n",
    "        return example\n",
    "\n",
    "    # pads a batch to largest # of visits / patient in the batch\n",
    "    # and largest # of concepts / visit along concept dim.\n",
    "    def collate(self, batch):\n",
    "        # first group along dict keys\n",
    "        batch_collated = {}\n",
    "        for k in batch[0].keys():\n",
    "            batch_collated[k] = [b[k] for b in batch]\n",
    "\n",
    "        keys = list(batch_collated.keys())\n",
    "        N = len(batch_collated['y'])\n",
    "\n",
    "        # each patient is a list of visits\n",
    "        max_num_visits = max([len(v) for v in batch_collated['visits']])\n",
    "        max_num_concepts = max(l for p in range(N) for l in [len(v) for v in batch_collated['visits'][p]])\n",
    "\n",
    "        concept_tensor = torch.full(\n",
    "            (N, max_num_visits, max_num_concepts),\n",
    "            0,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        \n",
    "        nums_tensor = torch.full(\n",
    "            (N, max_num_visits, max_num_concepts),\n",
    "            0,\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        nums_mask_tensor = torch.full(\n",
    "            (N, max_num_visits, max_num_concepts),\n",
    "            0,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "\n",
    "        times_tensor = torch.full((N, max_num_visits), -1, dtype=torch.long)\n",
    "        batch = batch_collated\n",
    "\n",
    "        lengths = torch.zeros(N)\n",
    "        for i, visit_list in enumerate(batch['visits']):\n",
    "            assert len(visit_list) == len(batch['unix_times'][i]), (\n",
    "                f\"Visits don't match! Got {len(visit_list)} visits and {len(batch['unix_times'][i])} times \"\n",
    "                f\"for batch element {i}\"\n",
    "            )\n",
    "            assert len(visit_list) == len(batch['visit_nums'][i]), (\n",
    "                f\"Visits don't match! Got {len(visit_list)} visits and {len(batch['visit_nums'][i])} nums \"\n",
    "                f\"for batch element {i}\"\n",
    "            )\n",
    "            num_visits = len(visit_list)  # visits of this patient we are including\n",
    "            lengths[i] = num_visits\n",
    "            for j, visit in enumerate(visit_list):\n",
    "                visit_size = len(batch['visits'][i][j])\n",
    "                visit_nums = batch['visit_nums'][i][j]\n",
    "                \n",
    "                assert(visit_size == len(visit))\n",
    "                assert visit_size == len(visit_nums), f\"Sizes differ! {visit_size} vs {len(visit_nums)}\"\n",
    "                \n",
    "                concept_tensor[i, j, : visit_size] = torch.Tensor(visit)\n",
    "                nums_tensor[i, j, : visit_size] = torch.Tensor(\n",
    "                    [0 if x is None else x for x in visit_nums]\n",
    "                )\n",
    "                nums_mask_tensor[i, j, : visit_size] = torch.Tensor(\n",
    "                    [x is not None for x in visit_nums]\n",
    "                )\n",
    "            times_tensor[i, :num_visits] = torch.Tensor(batch['unix_times'][i])\n",
    "            \n",
    "        batch.pop(\"unix_times\")\n",
    "        batch.pop(\"dates\")\n",
    "        # Another small correction:\n",
    "        batch.pop(\"tok_visits\")\n",
    "        batch.pop(\"visit_nums\")\n",
    "        batch[\"visits\"] = concept_tensor\n",
    "        batch[\"visits_values\"] = nums_tensor\n",
    "        batch[\"visits_values_mask\"] = nums_mask_tensor\n",
    "        batch[\"times\"] = times_tensor\n",
    "        batch[\"lengths\"] = lengths\n",
    "        for k,v in batch.items():\n",
    "            if not isinstance(v, torch.Tensor):\n",
    "                batch[k] = torch.tensor(v)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20282a18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 patients for empty visit lists\n",
      "peak memory: 839.12 MiB, increment: 442.97 MiB\n",
      "CPU times: user 2.24 s, sys: 303 ms, total: 2.54 s\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "AlteredODT = AlteredOMOPDatasetTorch(\"./altered_omop_reps/train/data.json\", max_num_visits=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13a5a5c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 16 batches of size 16 took the following time per batch:\n",
      "0:00:00.439344 Â± 0:00:00.038233 (0:00:00.394279-0:00:00.497850)\n",
      "\n",
      "Total batch size:\n",
      "40.1 MB Â± 19.7 MB (7.3 MB-88.7 MB)\n",
      "  Size of visits:\n",
      "    24.6 MB Â± 12.1 MB (4.5 MB-54.5 MB)\n",
      "  Size of static:\n",
      "    123 Bytes Â± 17 Bytes (56 Bytes-128 Bytes)\n",
      "  Size of y:\n",
      "    123 Bytes Â± 17 Bytes (56 Bytes-128 Bytes)\n",
      "  Size of visits_values:\n",
      "    12.3 MB Â± 6.1 MB (2.2 MB-27.3 MB)\n",
      "  Size of visits_values_mask:\n",
      "    3.1 MB Â± 1.5 MB (561.8 kB-6.8 MB)\n",
      "  Size of times:\n",
      "    80.9 kB Â± 36.2 kB (25.5 kB-131.1 kB)\n",
      "  Size of lengths:\n",
      "    61 Bytes Â± 8 Bytes (28 Bytes-64 Bytes)\n",
      "peak memory: 1786.18 MiB, increment: 145.68 MiB\n",
      "CPU times: user 1min, sys: 6.94 s, total: 1min 7s\n",
      "Wall time: 45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "profile_batch_iteration_speed_and_cost(\n",
    "    batch_size=16, pyd=AlteredODT, n_iter_samples=5, collate_fn=AlteredODT.collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f46ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
